{
  "metadata": {
    "name": "PySpark and SparkSQL basics",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n# Pull down data file from github and then load it from local host\nfrom pyspark import SparkFiles\n\nurl \u003d \"https://raw.githubusercontent.com/bethesdamd/mc_interview_exercise/master/master_list\"\nspark.sparkContext.addFile(url)\ndf \u003d spark.read.option(\"delimiter\", \"\\t\").csv(\"file://\"+SparkFiles.get(\"master_list\"), header\u003dTrue, inferSchema\u003dTrue)\n\ncountsByAuthor \u003d df.groupBy(\"Author\").count()\ncountsByAuthor.show(5)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n# schema\ndf.printSchema()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n# select columns\ndf.select(\u0027Title\u0027).show(5)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n# Filter\n\ndf.filter(df.Title\u003d\u003d\u00271984\u0027).show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n# Ordering\ndf.orderBy(df.Author).show(5)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n# select multiple columns\ndf.select(\u0027Title\u0027, \u0027Author\u0027).show(3)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n# building rdd then dataframe from raw data\nfrom pyspark.sql import Row\nl \u003d [(\u0027Ankit\u0027,25),(\u0027Jalfaizy\u0027,22),(\u0027saurabh\u0027,20),(\u0027Bala\u0027,26)]\nrdd \u003d sc.parallelize(l)\npeople \u003d rdd.map(lambda x: Row(name\u003dx[0], age\u003dint(x[1])))\nschemaPeople \u003d sqlContext.createDataFrame(people)\nschemaPeople.describe()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ntype(schemaPeople)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n## Creating DataFrame from RDD\nrdd \u003d sc.parallelize([\"b\", \"a\", \"c\"])\nsorted(rdd.map(lambda x: (x, 1)).collect())"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nschemaPeople.printSchema()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n# SQL\ndf.select(\u0027Author\u0027).show(5)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n# Hello World\nbig_list \u003d range(10000)\nrdd \u003d sc.parallelize(big_list, 2)\nodds \u003d rdd.filter(lambda x: x % 2 !\u003d 0)\nodds.take(15)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n#  count\ndf.count()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n# distinct\ndf.select(\u0027Author\u0027).distinct().take(4)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n# SparkSQL\ndf.registerTempTable(\u0027foo\u0027)\nsqlContext.sql(\"select * from foo\").show(5)   # dataframe\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n# Concat columns in SQL\n\ndf_title_author \u003d sqlContext.sql(\"select Title || Author as combined from foo\")\ndf_title_author.select(\u0027combined\u0027).distinct().show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n# group by and counting\n\nt \u003d df_title_author.groupBy(\"combined\").count()\nt.filter(t[\u0027count\u0027]\u003d\u003d5).show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}